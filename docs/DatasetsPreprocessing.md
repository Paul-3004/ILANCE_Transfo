# Datasets and Preprocessing
This document provides additionnal information of the datasets used to train and test the models as well as the preprocessing steps needed. 

## Generation of the raw datasets
The datasets are generated using the ILD full simulation programm,  ilcsoft v02-03-01 with geometry ILD_l5_o1_v02. Once a process is specified by the user, the program outputs $N$ events, in which the specified process has happened. The resulting particles first leave tracks in the vertex detector if charged, generating the _tracks_, then pass through the detector generating the _hits_. Thus after full simulations, each events in the raw datasets consists of tracks and hits in the detectors, which will be later on referred to as feats, and their associated Monte Carlo truth particles (MCTruth particles), referred to as labels. Both tracks and hits are contained in the feats array. 

Unprocessed feats entries are 10 dimensional vectors characterized by the following features:
- $E_{dep.}$: float, Energy deposit in GeV
- $(x,y, z)$: floats, Position of the hits in the calorimeters in mm.
- $t$: float, Time of detection in seconds.
- track: bool, True if the feat is corresponds to a track or a hit
- charge: int, charge of the particle passing through the detectors. Only available if track = 1
- $(p_x, p_y,  p_z)$: floats, momentum of the particle passing through the detectors in GeV. Computed from the track left by the particle, thus only available if track = 1.

> **NOTE:**
> Naturally, if a vector in an event is a track, it cannot have an energy deposit nor a corresponding position in the calorimetets. Thus for tracks, all entries for $E_{dep.}$ and $(x,y,  z)$ are 0.
> Similarly, charge and momentum cannot be inferred from only a hit, meaning that for the latter, the corresponding fields charge and $(p_x, p_y,  p_z)$ are also 0.


Unprocessed labels entries are 9D vectors, characterized by the features:
- hitid: int, unique identifiyer to the hit/track. Negative for tracks, positive otherwise.
- mcid: int, unique identifiyer for the MCTruth particle.
- pdg: int, used to identify the type of the MCTruth Particle as described by the [Particle Data Group Numbering Scheme](https://pdg.lbl.gov/2007/reviews/montecarlorpp.pdf)
- charge: int, charge of the MCTruth particle
- mass: float, mass of the MCTruth particle
- $(p_x, p_y,  p_z)$: momentum of the MCTruth particle
- status: not taken into consideration in this project.

As particles going through the detectors produce showers, there are in practice more hits than labels. However, since the programm keeps track of the Monte Carlo Truth particle from which derives the showers, each element of the feats is associated to its correponding element in the labels. In particular, hits from a same shower are associated to duplicates of the same MCTruth Particle label, leading to most of the labels contents being redundant for our use. To be clear, two hits $h_1$ and $h_2$ from the same shower will be associated to a duplicate of $l_1$, the label corresponding the MCTruth Particle generating the shower. This allows the feats and labels outer sizes to be the same, i.e. $N \times P_i$, where $N$ is the number of events generated by the simulation, and $P_i$ the number of hits + tracks per event $i$. (variable). 

> **NOTE:**
> Since the number of hits and tracks depend on the specific event, the formed arrays are not rectangular and is thus processed using the [Awkward Library](https://awkward-array.org/doc/main/).
> Labels is thus a $N\times P_i\times 9$ array, and feats $N \times P_i \times 10$

## Features of the formatted datasets

Features for the formatted feats were chosen to be $(E_{dep}, x, y, z,  [t])$, where [$t$] is used to indicate that this feature is optional. However, in all training done up to date, it has never been used such that the after selection hits are characterized by a 4D vector $(E_{dep.},x, y,  z)$.

> **NOTE:**
> This will be a non trivial entry only for hits, since all of these features are 0 for tracks. In fact, in the current implementation, keeping tracks is left as an option to the reader but have always been removed in all training done up until now. Indeed, since tracks and hits do not contain the same information, it seems difficult to treat them the same way as input to the model and a thoughtful implementation of the tracks as input is still missing.

Features for the formatted labels were chosen to be $(\textrm{charge}, |\textrm{ PDG}|,\, E, n_x,  n_y,  n_z)$, where $E$ is the energy of the MCTruth Particle in GeV and $(n_x,  n_y, n_z)$, its direction (momentum vector normalised to 1). Moreover, only one representative labels entry per cluster is kept, i.e. duplicata corresponding to hits belonging to the same shower are removed.

## Preprocessing
The purpose of the preprocessing is threefold. Firstly, the datasets in their raw form contain a lot of information that can be discarded as it will not be used, as it is the case for many features, or due to its redundancy, for the labels. Moreover, since Pytorch only deals with rectangular arrays, both feats and labels need to be padded with special `<pad>` tokens to achieve a fixed size $P$ as second dimension for all events. Lastly, as described in [Network Architectures](docs/NetworkArchitectures.md), `<bos>` and `<eos>` need to be added at the beginning and end of each event respectively, and additional features need to be added to the original feats/labels to mark them as `<sample>` tokens. 

### Feats Preprocessing
At the current state of the project, the preprocessing of the feats consists of the following steps:

1. Removing tracks from events
2. Keeping only the features $(E_{dep.},\, x, \, y, \, z)$ for the hits. 
3. Normalising both energy and position. Normalization is chosen such that the each feature in the **entire dataset** has a mean of 0 and Root Mean Square (RMS) value to one. Each feature $f$ after normalisation is thus sent to $f'$ by:
$$f_i' = \frac{1}{\sqrt{\textrm{RMS}(f)}}\left(f_i - \bar{f}\right), \textrm{ where RMS}(f) = \sqrt{\frac{\sum_{i}|f_i - \bar{f}|^2}{N-1}},$$
with
$$\bar{f} = \frac{1}{N}\sum_{i= 0}f_i,$$
the mean of the feature $f$ over all events. 
4. Addition of special symbols and tokens
    1. Addition of features marking hits as `<sample>` tokens with their corresponding special symbols (which depends on the implementation, see [Network Architectures](NetworkArchitectures.md) for details.)
    2. Addition of `<bos>` and `<eos>` tokens at beginning and end of each event respectively.
    3. Addition of `<pad>` tokens after `<eos>` to make all events with the same number of tokens. 

### Labels Preprocessing
At the current state of the project, the preprocessing of the labels consists of the following steps:

1. Removing labels entries corresponding to tracks (identified by a negative mcids or using the same mask as for the feats)
2. Removing duplicata to keep only one representative for each MCTruth Particle
3. Computing MCTruth Particles energies in GeV using available masses and momentums according to
$$E = \sqrt{m^2 + \vec{p}^2}$$
4. Normalising energies. Since the energy distribution of MCTruth particles in an event often have a long tail as $E$, the normalisation is performed on the logarithm base 10 of the energy. Again the (log10) energy for each entry, $E_i$, is sent to $E_i'$ according to
$$E_i' = \frac{1}{\sqrt{\textrm{RMS}(E)}}(E_i - \bar{E})$$
5. Normalising momentums to unit vectors.
6. Replacing PDG values by their absolute values
7. Removing all non-used features
8. Addition of special symbols and tokens
    1. Addition of features marking hits as `<sample>` tokens with their corresponding special symbols (which depends on the implementation, see [Network Architectures](NetworkArchitectures.md) for details.)
    2. Addition of `<bos>` and `<eos>` tokens at beginning and end of each event respectively.
    3. Addition of `<pad>` tokens after `<eos>` to make all events with the same number of tokens.
9. Translating charges and PDGs values to their corresponding indices (see [Network Architectures](NetworkArchitectures.md).)

## Datasets used for training
As we are still early in the developement of the project, the datasets are characterised by distinct clusters, minimising showers mixing.
### Single photons
This dataset is obtained by simulating single photons passing through the detectors. Their energies follow a logarithmic distribution between 10 and 100 GeV. The spherical angles of emission from the point of interaction are random and 100'000 events were simulated. 80% of those are used as training set, another 10% for the validation set and the last 10% for the test set. 

Due to interactions between the incoming photon and the detectors, pairs of particle/antiparticles generation and other radiations can occur. Some events thus contains more than 1 MCTruth Particle and not necessarily only photons, therefore affecting the models' performances (see [Results](Results.md) for a more in-depth discussion).

### Double photons
This dataset is obtained by simulating two photons passing through the detectors. Their energies and $\theta$ angle are fixed at 10 GeV and $85^{\circ}$ respectively. Furthermore, an opening angle of 100 mrad is kept between both photons. 

As for the single photon dataset, 100'000 events were generated. 80% forms the training set and the 20% left is shared equally between the validation set and the test set. Furthermore, physical interactions can produce other particles leading to several MCTruth particles other than photons. 

### Taus
This dataset is obtained by simulating 10 negative taus whose energies follow a logarithmic distribution between 10 and 100 GeV. All of the spherical angles associated to these initial taus are random and their decays are simulated by the Geant4 programm. 

A total of 1M events were generated, with about 90% forming the training set and the remaining being equally devided into a validation set and a test set. 



